{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "FineTuneFaceDetect.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihuzz/MobileNetV3_train-inference/blob/main/FineTuneFaceDetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6Yh2GymZI3J",
        "outputId": "31bf445c-c1af-4a7d-bdea-f0e475c9b20c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XXA-BkfwIMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e049e5-22e6-4401-f8c5-4b23e32f7150"
      },
      "source": [
        "%cd drive/MyDrive/Colab/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "db61d00a-30b2-4e58-b876-29c16c7956cc",
        "_cell_guid": "9216fafb-a118-47db-a921-b21b61c8568c",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "QG1Ff_4VL1FM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2884ffbc-b4ed-4e02-b5b4-931c761f82b0"
      },
      "source": [
        "!pip install pycocotools --quiet\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "# !git checkout v0.3.0\n",
        "\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v--R740-aPTo",
        "outputId": "46f22843-3c75-43e3-9ed3-ce28961ce6ad"
      },
      "source": [
        "!pip install albumentations==0.4.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e26f573b-ca02-417c-ad5b-8b4cf1488f98",
        "_cell_guid": "0a1fed4f-4d54-4455-ac33-677cf70665c0",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "abnP3UhhL1FO"
      },
      "source": [
        "# Basic python and ML Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# xml library for parsing xml files\n",
        "from xml.etree import ElementTree as et\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor,GeneralizedRCNN\n",
        "from torchvision.models.detection import FasterRCNN, generalized_rcnn\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "# these are the helper libraries imported.\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "29d8ca13-861b-4eb0-85ee-6efb3e2e2c97",
        "_cell_guid": "ab8513b7-05c6-43cb-8d59-e2d3ddaf6df7",
        "trusted": true,
        "id": "ZLpeLebzL1FP"
      },
      "source": [
        "build Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e17a226d-9222-4173-a747-2b08b0895da5",
        "_cell_guid": "8c4f7c3f-b387-4d62-9beb-afa23293e0f8",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "MfGwpeGfL1FQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e868f8-b89a-4c13-d878-eecde49e8f5f"
      },
      "source": [
        "# defining the files directory and testing directory\n",
        "files_dir = \"/content/drive/My Drive/Colab/Train\"\n",
        "test_dir = \"/content/drive/My Drive/Colab/DataSet_4_test\"\n",
        "\n",
        "\n",
        "class MyCustomDataSet(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, files_dir, width, height, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.files_dir = files_dir\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        \n",
        "        # sorting the images for consistency\n",
        "        # To get images, the extension of the filename is checked to be jpg\n",
        "        self.imgs = [image for image in sorted(os.listdir(files_dir))\n",
        "                        if image[-4:]=='.jpg']\n",
        "        \n",
        "        \n",
        "        # classes: 0 index is reserved for background\n",
        "        self.classes = [ 0, 'mom','Miha']\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_name = self.imgs[idx]\n",
        "        image_path = os.path.join(self.files_dir, img_name)\n",
        "\n",
        "        # reading the images and converting them to correct size and color    \n",
        "        img = cv2.imread(image_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "        # img_res = np.expand_dims(img_res, 0)\n",
        "        # img_res = img_res.transpose((2,0,1))\n",
        "        img_res /= 255.0\n",
        "        # img_res = np.expand_dims(img_res, 0)\n",
        "        # img_res = np.squeeze(img_res, 0)\n",
        "\n",
        "        # print(\"img shape :\", img.shape)\n",
        "        # print(\"img res shape :\", img_res.shape)\n",
        "        # annotation file\n",
        "        annot_filename = img_name[:-4] + '.xml'\n",
        "        annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        tree = et.parse(annot_file_path)\n",
        "        root = tree.getroot()\n",
        "        \n",
        "        # cv2 image gives size as height x width\n",
        "        wt = img.shape[1]\n",
        "        ht = img.shape[0]\n",
        "        \n",
        "        # box coordinates for xml files are extracted and corrected for image size given\n",
        "        for member in root.findall('object'):\n",
        "            labels.append(self.classes.index(member.find('name').text))\n",
        "            \n",
        "            # bounding box\n",
        "            xmin = int(member.find('bndbox').find('xmin').text)\n",
        "            xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            \n",
        "            ymin = int(member.find('bndbox').find('ymin').text)\n",
        "            ymax = int(member.find('bndbox').find('ymax').text)\n",
        "            \n",
        "            \n",
        "            xmin_corr = (xmin/wt) *self.width\n",
        "            xmax_corr = (xmax/wt) *self.width\n",
        "            ymin_corr = (ymin/ht) *self.height\n",
        "            ymax_corr = (ymax/ht) *self.height\n",
        "            \n",
        "            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "        \n",
        "        # convert boxes into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        \n",
        "        # getting the areas of the boxes\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        # image_id\n",
        "        image_id = torch.tensor([idx])\n",
        "        target[\"image_id\"] = image_id\n",
        "\n",
        "\n",
        "        if self.transforms:\n",
        "            \n",
        "            sample = self.transforms(image = img_res,\n",
        "                                     bboxes = target['boxes'],\n",
        "                                     labels = labels)\n",
        "            \n",
        "            img_res = sample['image']\n",
        "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            \n",
        "            \n",
        "            \n",
        "        return img_res, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "# check dataset\n",
        "dataset = MyCustomDataSet(files_dir, 224,224)\n",
        "print('length of dataset = ', len(dataset), '\\n')\n",
        "\n",
        "# getting the image and target for a test index.  Feel free to change the index.\n",
        "img, target = dataset[138]\n",
        "print(img.shape, '\\n',target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset =  650 \n",
            "\n",
            "(224, 224, 3) \n",
            " {'boxes': tensor([[ 46.2519,  78.1667, 163.8519, 177.9167]]), 'labels': tensor([2]), 'area': tensor([11730.6016]), 'iscrowd': tensor([0]), 'image_id': tensor([138])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f35a807e-309d-445a-b423-51a7d4c6545e",
        "_cell_guid": "23a94e27-0ec5-4cd5-8354-b69d12d570ab",
        "trusted": true,
        "id": "sUlxBdboL1FT"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "50139cfc-fed0-48ae-b9aa-d4e881d081b9",
        "_cell_guid": "d6739b97-600e-4416-bf71-30341c68909d",
        "trusted": true,
        "id": "XIRro6ZTL1FU"
      },
      "source": [
        "We will define a function for loading the model. We will call it later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "82c6de3c-e162-41a5-ac35-885c78213e0d",
        "_cell_guid": "6185996c-358e-49cc-99f8-ca8b95ddd2db",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "5iZleXQ_L1FV"
      },
      "source": [
        "def get_object_detection_Resnet50(num_classes):\n",
        "\n",
        "    # load a model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    \n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
        "\n",
        "    return model\n",
        "    \n",
        "\n",
        "def get_mobile_V3(num_classes):\n",
        "\n",
        "  model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "   \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e9c665d2-6fd0-499d-a577-94b3f0deb30c",
        "_cell_guid": "2dd340ab-1bc0-4d4e-9283-4039133e3a7c",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "5EwrEFeQL1FW"
      },
      "source": [
        "def get_transform(train):\n",
        "    \n",
        "    if train:\n",
        "        return A.Compose([\n",
        "                            \n",
        "                            A.HorizontalFlip(0.5),\n",
        "                          print(\"img.shape train :\", img.shape),\n",
        "                     # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "                            ToTensorV2(p=1.0),\n",
        "                          print(\"img.shape train :\", img.shape) \n",
        "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "    else:\n",
        "        return A.Compose([\n",
        "                            ToTensorV2(p=1.0),\n",
        "                            print(\"img.shape test :\", img.shape)\n",
        "                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f2b0c012-67cc-4285-9253-b4bdd9c1de6e",
        "_cell_guid": "a6501abd-a448-4ca7-a074-903910cc3429",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "Waz7IG_HL1FX"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = MyCustomDataSet(files_dir, 224, 224, transforms= get_transform(train=True))\n",
        "dataset_test = MyCustomDataSet(files_dir, 224, 224, transforms= get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(3)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "# train test split\n",
        "test_split = 0.2\n",
        "tsize = int(len(dataset)*test_split)\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n",
        "\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=1, shuffle=True, num_workers=4, \n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynE1PPof7iyc"
      },
      "source": [
        "#Define model parameters, choose cuda or not. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "11242c6b-3a1d-46ac-9de8-8aab68209a39",
        "_cell_guid": "b75c98c1-71e5-4551-a70d-5509a8bbd9c3",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "kCRnvxM9L1FX"
      },
      "source": [
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "num_classes = 3\n",
        "\n",
        "\n",
        "model= get_mobile_V3(num_classes)\n",
        "\n",
        "\n",
        "\n",
        "model.to(device, dtype=torch.float32)\n",
        "\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "optimizer2 = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4058f996-2e12-4af0-9b29-0124294f0c88",
        "_cell_guid": "42abef52-fdb9-4d45-aa19-9d797529c1a3",
        "trusted": true,
        "id": "MfK62F1hL1FX"
      },
      "source": [
        "Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a65fa99a-17cc-4768-a872-ca67e4dc3fb3",
        "_cell_guid": "80aebf81-b712-4f94-825c-6e85934ad321",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "hN6pS3beL1FX"
      },
      "source": [
        "# training for 10 epochs\n",
        "num_epochs = 6\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # training for one epoch\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6wwT83r6cRJ"
      },
      "source": [
        "%cd drive/MyDrive/Colab/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ5_FXvHOeZn"
      },
      "source": [
        "model.to(device='cuda', dtype=torch.float32)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # example_input = torch.rand(1, 3, 224, 224)\n",
        "    img_path = \"/content/drive/My Drive/Colab/Test_Data/image001.jpg\"\n",
        "    img = cv2.imread(img_path)\n",
        "    print(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    img = cv2.resize(img, (224,224), interpolation=cv2.INTER_CUBIC)\n",
        "    img = torch.tensor(img, dtype=torch.float32)\n",
        "    img /=255.0\n",
        "    img = img.permute(2,0,1)\n",
        "    # print(img)\n",
        "    img_path2 = \"/content/drive/My Drive/Colab/Test_Data/image487.jpg\"\n",
        "    img1 = cv2.imread(img_path2)\n",
        "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    img1 = cv2.resize(img1, (256,256), interpolation=cv2.INTER_CUBIC)\n",
        "    img1 = torch.tensor(img1, dtype=torch.float32)\n",
        "    img1 /=255.0\n",
        "    img1 = img1.permute(2,0,1)\n",
        "    # print(img1)\n",
        "    inplst = []\n",
        "    inplst.append(img)\n",
        "    inplst.append(img1)\n",
        "\n",
        "    torch.save(model, 'faster_320_mobilev3.pt')\n",
        "\n",
        "    traced_cpu = torch.jit.script(model, inplst)\n",
        "    torch.jit.save(traced_cpu, \"JIT_faster_mobileV3_large.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e24901c2-2ccf-4ed6-8367-d2332ff259aa",
        "_cell_guid": "6ad78a2a-7808-4651-b0d1-0653a293bcff",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "odu9sYapL1FZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}